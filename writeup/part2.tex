\section{Part II Results}
\label{sec:part2}

\paragraph{}
We investigate the expected weight of the MST in random graphs. We begin with a
hypothesis for an upper bound on the average MST weight of graphs created from
points randomly distributed in $k$-space. We then present and analyze our
empirical results.

% Graphs based on empirical and explanations for part 2 will be here.  We should
% include a subsection on our mathematical analysis of the upper bound and
% (hopefully) expected MST weight (could potentially plot these alongside the
% empircal data).

\subsection{Upper bound on average MST weight}
\paragraph{}
We derive an upper bound upper bound on the average MST weight of graphs created
from points randomly distributed in k-space. We begin by analyzing a graph in
2-D and later generalize to a graph in $k$ dimensions.

The MST of a graph with $|V|$ vertices will have $|V|-1$ edges. In order to
produce the \textit{maximum} weight MST for $|V|$ vertices, the $|V|-1$ edges
need to be as large as possible. We can maximize the MST weight by spreading
the vertices out as far as possible. We also maximize the MST by making all
edges the same length; this prevents trading off one edge of a longer distance
for one of a shorter distance. We do not prove this but offer it as a simple
hypothesis which forms the basis of our calculations.

If $|V|$ is a perfect square, then the $k$-space graph that maximizes any MST will form a
square grid with sides of length $\sqrt{|V|}$ and with the points spaced evenly
between its sides. The length of each edge is $\frac{1}{\sqrt{|V|}-1}$ so the
total MST weight is $\frac{|V|-1}{\sqrt{|V|}-1}$. Performing polynomial long
division on this fraction gives us $|V|^{\frac{1}{2}}+1$ as an alternate form
for the MST weight.

We can perform the same analysis on higher dimensionality spaces (such as 3-D
and 4-D) and get analogous results. We omit the derivations for brevity, but
generalizing the equation to $k$ dimensions gives a maximum MST weight for graph
with $|V|$ vertices as:
\[ |V|^{\frac{k-1}{k}} + |V|^{\frac{k-2}{k}} + \cdots + |V|^{\frac{1}{k}} +
|V|^{\frac{0}{k}} \]
In Section~\ref{sec:part2:compare} we compare our empirical results with this
theoretical upper bound.

\subsection{Empirical results}
\paragraph{}
For these results, we generated inputs with fifteen decimal places of precision
and recorded MST weights to three decimal places of precision.  This helped
ensure that we considered a very wide range of the possible values within the
unit range.  Section~\ref{sec:deep:graphgen} provides more details about how
graphs were generated.

Figure~\ref{fig:part2} shows experimentally determined average MST weights for
four types of graphs.  These types are random unit edge weights (labeled
``Edge'') and edge weights based on random vertex placement in $k$-space for
$k=2$, $k=3$, and $k=4$.  The average weights for each of these four dataset was
sampled at a variety of vertex quantities.  In particular, results are shown for
all $|V|$ between $2$ and $8192$ (inclusive) which are powers of $1.5$ or $2$.
Furthermore, for each dataset, the weights for \textit{each} of these $34$
distinct $|V|$ values is determined by an average of at least $50$ random
graphs.  In total, over $10,000$ graphs were generated and analyzed to produce
this plot; more details about how these graphs were generated is available in
Section~\ref{sec:deep:graphgen}. We also computed $99.95\%$ confidence intervals
of these averages but omit them from the figure because they are so small.  The
tiny size of these ranges allows us to say with high confidence that our
measurements are statistically significant.

\begin{figure}[t!]
\centering
\includegraphics[width=0.50\textwidth]{figures/part2.pdf}
\caption{Empirical data showing how average MST weight increases with $|V|$
  for complete graphs.}
\label{fig:part2}
\end{figure}

In order to accurately characterize how these MSTs grow as a function of $n$, we
used the Marquardt-Levenberg algorithm to fit a range of growth rate functions
to each dataset.  We covered a broad range of function classes from constant to
exponential.  Figure~\ref{fig:part2-fit} shows how well each of these growth
rates performs for each dataset.  These candidate growth rates we fit are ($n =
|V|$):
\begin{eqnarray*}
constant&:& c \\
log&:& m \cdot log_2(n) + c \\
polynomial / log&:& m \cdot \frac{n^p}{log_2(n)} + c \\
polynomial&:& m \cdot n^p + c \\
polynomial \cdot log&:& m \cdot n^p \cdot log_2(n) + c \\
exponential&:& m \cdot b^n + c
\end{eqnarray*}

\begin{figure*}[htb!]
\centering
\mbox{
\subfigure[Best-fit lines for random edge weights in the range $0$ to $1$ inclusive.
  ]{\label{fig:part2-fit-edge}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-edge}}
\quad
\subfigure[Best-fit lines for random vertex positions in 2-D unit space.
  ]{\label{fig:part2-fit-loc2}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-loc2}}
}
\mbox{
\subfigure[Best-fit lines for random vertex positions in 3-D unit space.
  ]{\label{fig:part2-fit-loc3}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-loc3}}
\quad
\subfigure[Best-fit lines for random vertex positions in 4-D unit space.
  ]{\label{fig:part2-fit-loc4}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-loc4}}
}
\caption{Comparison of best-fit lines to empirical data.}
\label{fig:part2-fit}
\end{figure*}

The fit for random edge weights was not as strong as the other fits, but the
best candidate was a very small logarithmic function: $0.004 \cdot log_2(|V|) +
1.1$.  The $99.95\%$ confidence interval for the data making up this result is
also shown in Figure~\ref{fig:part2-fit-edge} because it is the only set of data
for which the confidence interval is large enough to be visible (mostly because
the values are quite small for this set of data).

We found that a sublinear polynomial best fits the $k$-space data.
Figures~\ref{fig:part2-fit-loc2}, \ref{fig:part2-fit-loc3}, and
\ref{fig:part2-fit-loc4} show the fits for the $k$-space data.  The
polynomial$/$log and polynomial$\cdot$log fits are quite as good as the
sublinear polynomial fit. One reason for this similarity is that $\log |V|$
and $|V|^\epsilon$ both grow very slowly and look very similar over this range
of values. The constant, log, and exponentials fits are poor.
Table~\ref{table:part2-comparison} shows the values for the best-fit growth
rates for each $k$-space data set.

% Describe our best guesses for how each grows (show the best best-fit equations
% with values filled in as on the plots).  Describe why we chose that fit.  Try to
% explain what the heck is going on with the random edge weights one.  Make sure
% to explain that poly/log, poly, and poly*log are all very good fits - they are
% almost indistinguishable on the graph but that poly is best; also not that
% const, log, and exp are clearly bad fits.

\subsection{Comparison with Theoretical}
\label{sec:part2:compare}
\paragraph{}
We compare our empirical results with the theoretical upper bound we presented
earlier. The intuition is that as $|V|$ gets larger it will more closely resemble
the $k$-dimensional grid. With many points placed at random in a limited space,
the distribution of distances between neighboring points becomes more uniform,
and more points fill out the space to the edges. In
Table~\ref{table:part2-comparison} we list the curve that best fits our
empirical results, and next to it we list the derived upper bound.  This derived
upper bound is plotted against the empirical results in
Figure~\ref{fig:part2-with-theoretical}.

\begin{table}
\begin{tabular}{|c|c|c|}
\hline
k&Empirical best fit&Theoretical upper bound\\
\hline
1&$1$&$1$\\
2&$0.8*|V|^{0.5} + -0.5$&$|V|^{\frac{1}{2}} + 1$\\
3&$0.8*|V|^{0.6} + -0.6$&$|V|^{\frac{2}{3}} + |V|^{\frac{1}{3}} + 1$\\
4&$0.9*|V|^{0.7} + -0.7$&$|V|^{\frac{3}{4}} + |V|^{\frac{2}{4}} + |V|^{\frac{1}{4}} + 1$\\
\hline
\end{tabular}
\caption{Comparison of empirically-determined best-fit growth rates to the
  theoretical upper bound on MST weight for $k$-space data.}
\label{table:part2-comparison}
\end{table}

We see that the best curve is lower than but follows the polynomial upper
bound. For example, the best 4-D curve had a $0.7$ degree polynomial and the
upper bound had a $0.75$ degree polynomial. These tests support our
hypothesis.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.50\textwidth]{figures/part2-with-theoretical.pdf}
\caption{Comparison of theoretical upper bounds with empirical data.}
\label{fig:part2-with-theoretical}
\end{figure}
