\section{Part II Results}
\label{sec:part2}

\paragraph{}
We investigate the expected weight of the MST in random graphs. We begin
with a discussion of the mathematical upper bound on average MST weight for graphs
created by points in k-space. We then discuss out empirical attempts to
estimate the expected weight.

% Graphs based on empirical and explanations for part 2 will be here.  We should
% include a subsection on our mathematical analysis of the upper bound and
% (hopefully) expected MST weight (could potentially plot these alongside the
% empircal data).

\subsection{Upper bound on average MST weight}
\paragraph{}
For graphs created by points in a unit space we can derive an upper bound on
MST weight. We begin by analyzing a graph in 2-D and generalize that to a
graph in k dimensions.

A graph of n nodes will have an MST of n-1 edges. In order to produce
the maximum weighted MST of n nodes, the n-1 edges need to be as large as
possible. Because the MST picks the smallest n-1 edges that do not produce
cycles, we can maximize the MST's weight by spreading the nodes out as far
as possible. We also maximize the MST by making all edges of equal length,
thus not allowing any tree to trade off one edge of a longer distance with
one of a shorter distance. We do not prove this but offer it as some
simple reasoning for our calculations.

If n is a perfect square, then the graph that
maximizes any MST will be a $\sqrt{n} \times \sqrt{n}$ grid with the
points spaced evenly from 0 to 1 in both dimensions. The length of each
edge is $\frac{1}{\sqrt{n}-1}$ so the total MST weight is
$\frac{n-1}{\sqrt{n}-1}$. Performing polynomial long division on this
fraction gives us $n^{\frac{1}{2}}+1$ as an alternate form for the MST
weight.

We can perform the exact same analysis on graphs in 3-D and 4-D and get
analagous results. We omit the derivations for brevity, but generalizing
the equation to k dimensions gives a maximum MST weight for an n node
graph as:
\[ n^{\frac{k-1}{k}} + n^{\frac{k-2}{k}} + \cdots + n^{\frac{1}{k}} +
n^{\frac{0}{k}} \]
We will later compare our empirical results with this theoretical upper
bound.

\subsection{Empirical results}
\paragraph{}
Figure~\ref{fig:part2} shows empirical MST weight data. For each type of
graph and for all values of $|V|$
between $2$ and $8192$ (inclusive) which are powers of $1.5$ or $2$ it
plots the average MST weight.  For each type of graph, the
weights for each of these 34 $|V|$ values is determined by
an average of at least 50 random graphs with that value of $|V|$.  In
total, over $10,000$ graphs
were generated and analyzed to produce this plot; more details about how
these graphs were generated is available in Section~\ref{sec:tbd}.  We
also computed $99.95\%$ confidence intervals of these averages but omit
them from the figure because they are so small.  The tiny size of these
ranges allows us to say with high confidence that our results here are
statistically significant.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.50\textwidth]{figures/part2.pdf}
\caption{Empirical data showing how the average MST weight increase with $|V|$
  for complete graphs.}
\label{fig:part2}
\end{figure}

\paragraph{}
We compared several curves to the MST data for each graph type. We used the
Marquardt-Levenberg algorithm to fit a range of growth rates, from constant
to exponential, to each set of data.
Figure~\ref{fig:part2-fit} shows how each of these fits performs for each case.
The equations we fit are ($n = |V|$):
\begin{eqnarray*}
constant&:& c \\
log&:& m \cdot log_2(n) + c \\
polynomial / log&:& m \cdot \frac{n^p}{log_2(n)} + c \\
polynomial&:& m \cdot n^p + c \\
polynomial \cdot log&:& m \cdot n^p \cdot log_2(n) + c \\
exponential&:& m \cdot b^n + c
\end{eqnarray*}

\begin{figure*}[htb!]
\centering
\mbox{
\subfigure[Best-fit lines for random edge weights in the range $0$ to $1$ inclusive.
  ]{\label{fig:part2-fit-edge}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-edge}}
\quad
\subfigure[Best-fit lines for random vertex positions in 2-D unit space.
  ]{\label{fig:part2-fit-loc2}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-loc2}}
}
\mbox{
\subfigure[Best-fit lines for random vertex positions in 3-D unit space.
  ]{\label{fig:part2-fit-loc3}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-loc3}}
\quad
\subfigure[Best-fit lines for random vertex positions in 4-D unit space.
  ]{\label{fig:part2-fit-loc4}\includegraphics[width=0.5\linewidth,angle=0]{figures/part2-fit-loc4}}
}
\label{fig:part2-fit}
\end{figure*}

\paragraph{}
For each graph type we found that a sublinear polynomial best fit
the data. Very close to that line were also the polynomial/log and
polynomial*log lines. The reasoning for these other two functions being
similar is that $\log n$ and $n^\epsilon$ both grow very slowly and look
very similar over a significant range of values. Therefore $n^y \approx
n^y * \frac{n^\epsilon}{\log n}$ and $n^y \approx n^y * \frac{\log
  n}{n^\epsilon}$ over a certain range. The functions that are clearly not
close are constant, log, and exponential.

See table\ref{??} below for lines that best fit each data set.

% Describe our best guesses for how each grows (show the best best-fit equations
% with values filled in as on the plots).  Describe why we chose that fit.  Try to
% explain what the heck is going on with the random edge weights one.  Make sure
% to explain that poly/log, poly, and poly*log are all very good fits - they are
% almost indistinguishable on the graph but that poly is best; also not that
% const, log, and exp are clearly bad fits.

\subsection{Comparison of empirical and theoretical results}
\paragraph{}
We compare our empirical results for the expected MST weight with the
mathematical upper bound we derived earlier. The intuition is that as n
gets larger, it will more closely resemble the k-dimensional grid. With
many points placed at random in a limited space, the distribution of
distances between neighboring points becomes more uniform, and more points
fill out the space to the edges. Below we
list the curve that best fits our empirical results, and next to it we
list the derived upper bound.  This derived upper bound is plotted against the
empirical results in Figure~\ref{fig:part2-with-theoretical}.

\begin{tabular}{|c|c|c|}
\hline
Dimension&Empirical best fit&Theoretical upper bound\\
\hline
1&$1$&$1$\\
2&$0.8*n^{0.5} + -0.5$&$n^{\frac{1}{2}} + 1$\\
3&$0.8*n^{0.6} + -0.6$&$n^{\frac{2}{3}} + n^{\frac{1}{3}} + 1$\\
4&$0.9*n^{0.7} + -0.7$&$n^{\frac{3}{4}} + n^{\frac{2}{4}} + n^{\frac{1}{4}} + 1$\\
\hline
\end{tabular}

We see that the best curve is lower than but follows the polynomial upper
bound. For example, the best 4-D curve had a 0.7 degree polynomial and the
upper bound had a 0.75 degree polynomial. These tests confirm our
hypothesis.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.50\textwidth]{figures/part2-with-theoretical.pdf}
\caption{Comparison of theoretical upper bounds with empirical data.}
\label{fig:part2-with-theoretical}
\end{figure}
