\subsection{Algorithm Choice}
\label{sec:deep:algchoice}
This section discusses algorithms we implemented, effective tweaks, and
theoretical versus empirical growth rates.  All versions of all algorithms
discussed here and the results presented here are based on data from a battery
of $71$ $|V|$ and $|E|$ pairs which cover $|E|:|V|$ ratios of $5, 50, 150, and
1500$.  The magnitude of edges spanned from $100,000$ to $10,000,000$.  Five
different graphs with each of those $71$ $|V|$ and $|E|$ pairs was generated for
a total of $355$ inputs.  For each version of the algorithm, we ran it five
times on each of these inputs for a total of $1,755$ trials each.  The $25$
results for each $|V|$ and $|E|$ pair were statistically significant - though
not shown, the $99.95\%$ confidence intervals for all these results are quite
close to the mean runtimes.

%Discuss the algorithms we tried, our experience with each, and the tweaks we
%made (both successful and not).  Provide empirical data (graphs) which compare
%these algorithms and to strengthen our discussion of the tweaks we made and
%their relative performance.
%\paragraph{}
%We should probably consider one algorithm at a time, then compare the best
%version of each in a final subsubsection.  Include a little discussion as
%whether these algorithms running times match their asymptotic complexity.

\subsubsection{Prim's Algorithm}
\paragraph{}
We implemented two versions of Prim's algorithm.  One version, ``PH'' was a
heap-based version of Prim's.  The heap was used to quickly determine which
``frontier'' edge had the minimum weight.  The \texttt{decreaseKey} method needs
to be efficient for this algorithm to work well - edges on the frontier are
often updated.  Fibonacci heaps are a complicated data structure which promise
to use only constant amortized time for this operation, but in practice it tends
to be a complicated data structure with high constant costs.  Thus we chose to
use a pairing heaps data structure which is likely to offer similar guarantees
with much less complexity - some studies\cite{moret} have shown they work well
in practice too.  The theoretical complexity of this algorithm is $O(|E| \cdot
log(|V|))$.

The other version, ``PD'', of Prim's algorithm at first seems too primitive, but
turns out to be very applicable on very dense graphs.  It is implemented with an
adjacency matrix which allows it to check edge cost or existence very cheaply.
The algorithm considers the edges attached to each vertex when the vertex is
added to the MST - i.e., it looks at what new vertices it might be able to get
to which are not in the MST already.  This must be done for each vertex, so this
leads to an algorithm whose theoretical complexity is $O(|V|^2)$ - thus for
dense graphs it is actually a linear algorithm in terms of $|E|$!

There were a few implementation tricks which led to significant speedups.  For
``PH'', we modified a multi-pass pairing heap from \cite{rui} to eliminate
pointer passing (essentially making it a static class) and crucially
preallocating memory for all of the nodes it would need rather than continuously
allocating and deallocating them.  In retrospect, we should have used a more
recent incarnation of the pairing heap data structure which gives limits
\texttt{decreaseKey}'s cost to $O(log(log(n)))$\cite{elmasry}.  We also
discovered a simple trick for improving ``PD'': at a cost of allocating twice as
much memory, we store all edges both in the forwards $(u, v)$ and reverse $(v,
u)$ direction.  This makes lookups faster since it avoids expensive branches to
ensure the appropriate vertex is used as $u$ in the original scheme which only
stored edges in one direction.

We found that a best-fit growth rate analysis on empirical data for ``PH''
roughly matched the theoretical estimate: the best-fit was a polynomial $\cdot$
log function $1.0 * |E|^{0.9} * log_2(|E|) + -0.010$.  Stability issues in the
Marquardt-Levenberg algorithm and our data for ``PD'' forced us to find a
reasonable fit by hand.  The best-fit for ``PD'' also roughly matched the
theoretical estimate: $0.0002 \cdot |V|^2$ nicely fits the data.  The empirical
data and best-fit lines for these are shown in
Figures~\ref{fig:p1-fit-prim-heap} and ~\ref{fig:p1-fit-prim-dense}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.50\textwidth]{figures/p1-fit-prim-heap.pdf}
\caption{Empirical data and best-fit line for Prim Heap.}
\label{fig:p1-fit-prim-heap}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.50\textwidth]{figures/p1-fit-prim-dense.pdf}
\caption{Empirical data and best-fit line for Prim Dense.}
\label{fig:p1-fit-prim-dense}
\end{figure}

\subsubsection{Kruskal's Algorithm}
\paragraph{}
We implemented multiple versions of Kruskal's and had surprising
results. One major difference is in the Union-Find data structure that
provides the fundamental abilities to find the connected component of a
given vertex and to combine the connected components of two vertices. The
textbook describes one implementation that maintains an explicit set of
vertices in a connected component. When Kruskal's algorithm adds an edge,
unioning the sets requires copying the members in the smaller connected
component into the larger connected component's set. A second version
maintains an implicit set through a chain of ``parent
pointers''. One optimization on this second approach collapses this chain
each time it is traversed by updating vertices' parent pointers.

Surprisingly, the explicitly maintained set performed faster than the
implicit version, which differs from the theoretical results in the
book. The reason appeared to
be that the explicit set had much better cache performance on large
graphs. The find operation of the
implicit set required following a chain of parent pointers that jumped
all around the array of nodes. The explicit set did not require this
hopping around through memory. Even with multiple optimizations of the
second approach, the explicitly maintained set was faster.

The other major tradeoff in Kruskal's we found was how much of the list of
edges we sorted at a time. Because Kruskal's walks through edges in sorted
order, and ends when all nodes are connected, there is no need to sort any
edges not included in the final MST. Because sorting the edges is the
largest expenditure of time in Kruskal's, it is a big win to eliminate
unnecessary sorting. However, this is tricky to do because you can only
guess at the beginning how much of the array will need to be sorted. Then
you have to keep track of the sorted portion, and sort more if you move
past it. We found a good solution was to sort differing percentages of the
edge array based on the density of the graph. The optimal values found are
listed in Table \ref{table:kruskal-sort-percent}.

We found that a best-fit growth rate analysis on empirical data for ``KPS''
roughly matched the theoretical estimate: the best-fit was a polynomial $\cdot$
log function $1.9 * |E|^{1.0} * log_2(|E|) + -0.0$.  The empirical data and
best-fit line is shown in Figures~\ref{fig:p1-fit-kruskal}.

\begin{table}
\begin{tabular}{|c|c|}
\hline
density&\% of edges to sort at a time\\
\hline
$\frac{m}{n} < 10$& 100\% \\
$10 \leq \frac{m}{n} < 50$& 20\% \\
$50 \leq \frac{m}{n}$& 10\% \\
\hline
\end{tabular}
\caption{Size of a portion of the edge list we sort at a time.}
\label{table:kruskal-sort-percent}
\end{table}

\begin{figure}[htb]
\centering
\includegraphics[width=0.50\textwidth]{figures/p1-fit-kruskal.pdf}
\caption{Empirical data and best-fit line for Kruskal.}
\label{fig:p1-fit-kruskal}
\end{figure}
